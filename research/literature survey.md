# RAFT


![[raft.pdf]]

# summary
RAFT (Retrieval Augmented Fine-Tuning) is a technique that enhances the knowledge and performance of large language models by integrating retrieval mechanisms during the fine-tuning phase. Unlike Retrieval-Augmented Generation (RAG), which retrieves relevant documents at inference time, RAFT incorporates retrieved information directly into the model's training process. This approach allows the model to internalize and store knowledge from external sources, improving its ability to answer questions and generate content without the need for real-time retrieval during inference. By focusing on fine-tuning with relevant information, RAFT reduces latency and enhances the efficiency and effectiveness of language models in handling domain-specific queries. This method is particularly advantageous for scenarios where real-time retrieval might be computationally intensive or impractical.




# injecting new knowledge via fine-tuning


![[2404.00213v2.pdf]]



# summary
In summary, fact-based dataset generation is a structured and systematic approach to creating training data that enhances the learning and performance of language models by ensuring comprehensive and balanced coverage of all significant facts from the source material.






# fine-tuning limitations

![[2405.05904v2.pdf]]